{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP Airtable CSV → Canonical Holon JSON (Row-wise SchemaType Detection)\n",
    "\n",
    "Created: 2025-08-16T20:43:45\n",
    "\n",
    "**Change:** Rows are classified **per row** using the CSV `type` column.\n",
    "- author JSON `type: \"#{type}\"` from CSV `type` column\n",
    "\n",
    "\n",
    "Also preserved:\n",
    "- `ComponentOf`, `extends`, `uses_key_rule` → emitted as **relationships**\n",
    "- No `key` inside `properties`\n",
    "- Two export modes: `\"single\"` or `\"by-file\"`\n"
   ],
   "id": "66be6e420aafc5d5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 1) Configuration",
   "id": "3b4a875e50957305"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:58:12.210938Z",
     "start_time": "2025-08-18T01:58:12.204878Z"
    }
   },
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "INPUT_DIR = Path(\"./inputs\"); INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR = Path(\"./outputs\"); OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose exactly one export mode: \"single\" or \"by-file\"\n",
    "EXPORT_MODE = \"by-file\"   # change to \"by-file\" for 1 JSON per CSV, \"single\" for one JSON with all types\n",
    "\n",
    "# Optional future schema path (unused here)\n",
    "JSON_SCHEMA_PATH = None\n",
    "\n",
    "EXPORT_META = {\n",
    "    \"generator\": \"MAP CSV→JSON Notebook (row-wise SchemaType detection)\",\n",
    "    \"generated_at\": datetime.datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"export_mode\": EXPORT_MODE,\n",
    "}\n",
    "\n",
    "print(\"INPUT_DIR:\", INPUT_DIR.resolve())\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR.resolve())\n",
    "print(\"EXPORT_MODE:\", EXPORT_MODE)\n"
   ],
   "id": "81c37938421abac1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIR: /Users/matrix/Documents/coding-projects/memetic-activation-platform/catalist-type-conversion-notebook/inputs\n",
      "OUTPUT_DIR: /Users/matrix/Documents/coding-projects/memetic-activation-platform/catalist-type-conversion-notebook/outputs\n",
      "EXPORT_MODE: by-file\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 2) Upload or Discover CSVs",
   "id": "76ef591fb6dea2a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:58:12.724635Z",
     "start_time": "2025-08-18T01:58:12.218671Z"
    }
   },
   "source": "\nimport pandas as pd\ntry:\n    import ipywidgets as widgets\n    from IPython.display import display\n    HAS_WIDGETS = True\nexcept Exception:\n    HAS_WIDGETS = False\n\nif HAS_WIDGETS:\n    upload = widgets.FileUpload(accept='.csv', multiple=True)\n    display(upload)\n    def _save_upload(change):\n        for name, item in upload.value.items():\n            (INPUT_DIR / name).write_bytes(item[\"content\"])\n        print(f\"Saved {len(upload.value)} file(s) to {INPUT_DIR.resolve()}\")\n    upload.observe(_save_upload, names='value')\nelse:\n    print(\"ipywidgets not available; place CSVs directly in:\", INPUT_DIR.resolve())\n\n# Auto-copy previously attached CSVs if present\nATTACHED = [\n    \"MAP Meta-Schema 0.0.3-metaschema-abstract-value-types.csv\",\n    \"MAP Meta-Schema 0.0.3-metaschema-concrete-value-types.csv\",\n    \"MAP Meta-Schema 0.0.3-metaschema-keyrules-schema.csv\",\n    \"MAP Meta-Schema 0.0.3-metaschema-property-types.csv\",\n    \"MAP Meta-Schema 0.0.3-metaschema-relationship-types.csv\",\n    \"MAP Meta-Schema 0.0.3-metaschema-root.csv\",\n]\nfrom pathlib import Path as _Path\nsrc_dir = _Path(\"/mnt/data\")\nfor fname in ATTACHED:\n    src = src_dir / fname\n    if src.exists():\n        dst = INPUT_DIR / fname\n        if not dst.exists():\n            dst.write_bytes(src.read_bytes())\n            print(\"Copied attached:\", fname)\n\nCSV_FILES = sorted(INPUT_DIR.glob(\"*.csv\"))\nprint(\"Discovered\", len(CSV_FILES), \"CSV file(s):\")\nfor p in CSV_FILES:\n    print(\" -\", p.name)\n",
   "id": "a03342738f89df72",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileUpload(value=(), accept='.csv', description='Upload', multiple=True)"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd376191774442d1b4fa4e9db94cd1d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 6 CSV file(s):\n",
      " - MAP Meta-Schema 0.0.3-metaschema-abstract-value-types.csv\n",
      " - MAP Meta-Schema 0.0.3-metaschema-concrete-value-types.csv\n",
      " - MAP Meta-Schema 0.0.3-metaschema-keyrules-schema.csv\n",
      " - MAP Meta-Schema 0.0.3-metaschema-property-types.csv\n",
      " - MAP Meta-Schema 0.0.3-metaschema-relationship-types.csv\n",
      " - MAP Meta-Schema 0.0.3-metaschema-root.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 3) Helpers",
   "id": "86e23a4797324722"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:58:12.745994Z",
     "start_time": "2025-08-18T01:58:12.738342Z"
    }
   },
   "source": [
    "\n",
    "import re, json\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "def list_norm(v) -> List[str]:\n",
    "    if pd.isna(v) or v == \"\": return []\n",
    "    if isinstance(v, list): return [str(x).strip() for x in v if str(x).strip()]\n",
    "    parts = re.split(r\"[;,]\", str(v))\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def ensure_hash_key(key: str) -> str:\n",
    "    if not key: return key\n",
    "    if key.startswith((\"#\",\"id:\",\"@\",\"ext:\")): return key\n",
    "    return f\"#{key}\"\n",
    "\n",
    "def ref(key: str) -> Dict[str, str]:\n",
    "    return {\"$ref\": ensure_hash_key(str(key))}\n",
    "\n",
    "def scalarize(v: Any):\n",
    "    if v is None: return None\n",
    "    if isinstance(v, (bool, int, float, str)): return v\n",
    "    if isinstance(v, list): return [scalarize(x) for x in v]\n",
    "    return str(v)\n",
    "\n",
    "def col(df: pd.DataFrame, *candidates) -> Optional[str]:\n",
    "    m = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c.lower() in m: return m[c.lower()]\n",
    "    return None\n",
    "\n",
    "REL_COL_TO_NAME = {\n",
    "    \"componentof\": \"ComponentOf\",\n",
    "    \"extends\": \"Extends\",\n",
    "    \"uses_key_rule\": \"UsesKeyRule\",\n",
    "    \"inverseof\": \"InverseOf\",\n",
    "    \"inverse_of\": \"InverseOf\",\n",
    "    \"sourcetype\": \"SourceType\",\n",
    "    \"targettype\": \"TargetType\"\n",
    "}\n",
    "\n",
    "RESERVED_COLS = {\n",
    "    \"key\", \"type\", \"metaschema_partition\",\n",
    "    \"componentof\", \"extends\", \"uses_key_rule\",\n",
    "    \"inverseof\", \"inverse_of\",\n",
    "    \"sourcetype\", \"targettype\"\n",
    "}\n"
   ],
   "id": "aebbc4a0c2a34e1a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 4) Holon model",
   "id": "87b83d88c0bc8ff1"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:58:12.784610Z",
     "start_time": "2025-08-18T01:58:12.777458Z"
    }
   },
   "source": [
    "\n",
    "class Holon:\n",
    "    def __init__(self, type_ref: str, key: Optional[str] = None):\n",
    "        self.key = key\n",
    "        self.type = ensure_hash_key(type_ref)\n",
    "        self.properties: Dict[str, Any] = {}\n",
    "        self.relationships: List[Dict[str, Any]] = []\n",
    "\n",
    "    def add_property(self, name: str, value: Any):\n",
    "        if value is None: return self\n",
    "        self.properties[name] = scalarize(value); return self\n",
    "\n",
    "    def add_relationship(self, name: str, targets: Any):\n",
    "        self.relationships.append({\"name\": name, \"target\": targets}); return self\n",
    "\n",
    "    def to_json(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to JSON with field order: key, type, properties, relationships.\"\"\"\n",
    "        obj: Dict[str, Any] = {}\n",
    "        # 1) key\n",
    "        if self.key is not None:\n",
    "            obj[\"key\"] = self.key\n",
    "        # 2) type\n",
    "        obj[\"type\"] = self.type\n",
    "        # 3) properties (always present)\n",
    "        obj[\"properties\"] = self.properties\n",
    "        # 4) relationships (only if present)\n",
    "        if self.relationships:\n",
    "            obj[\"relationships\"] = self.relationships\n",
    "        return obj\n"
   ],
   "id": "e9c7f639f9f6d8ba",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 5) Optional indexes: key rules & inverse mapping",
   "id": "78c4ab2b50698f9e"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:58:12.794406Z",
     "start_time": "2025-08-18T01:58:12.789447Z"
    }
   },
   "source": "\nfrom typing import Tuple\n\nclass KeyRuleEngine:\n    def __init__(self):\n        self.rules: Dict[str, Optional[Tuple[str, List[str]]]] = {}\n\n    def register(self, holon_type_key: str, fmt: Optional[str], prop_names: Optional[List[str]]):\n        self.rules[ensure_hash_key(holon_type_key)] = (fmt, prop_names) if fmt else None\n\n    def is_keyed(self, holon_type_key: str) -> bool:\n        return self.rules.get(ensure_hash_key(holon_type_key), (\"$0\", [\"type_name\"])) is not None\n\n    def derive(self, holon_type_key: str, properties: Dict[str, Any]) -> Optional[str]:\n        rule = self.rules.get(ensure_hash_key(holon_type_key))\n        if rule is None: return None\n        fmt, names = rule\n        names = names or []\n        key = fmt\n        for i, n in enumerate(names):\n            key = key.replace(f\"${i}\", str(properties.get(n, \"\")))\n        return key\n\n    def validate(self, h: Holon) -> List[str]:\n        errs: List[str] = []\n        keyed = self.is_keyed(h.type)\n        if keyed and not h.key: errs.append(f\"{h.type} requires a key but none provided.\")\n        if (not keyed) and h.key is not None: errs.append(f\"{h.type} is keyless but a key was provided.\")\n        if keyed and h.key:\n            derived = self.derive(h.type, h.properties) or \"\"\n            if derived and h.key != derived:\n                errs.append(f\"Key '{h.key}' does not match derived '{derived}' for {h.type}.\")\n        return errs\n\nclass InverseIndex:\n    def __init__(self):\n        self.inverse_of: Dict[str, str] = {}\n    def declare_pair(self, declared: str, inverse: str):\n        if declared and inverse: self.inverse_of[str(inverse).strip()] = str(declared).strip()\n    def rewrite(self, name: str) -> str:\n        return self.inverse_of.get(name, name)\n\ndef rewrite_relationship_names(holon: Holon, inv: InverseIndex):\n    for r in holon.relationships: r[\"name\"] = inv.rewrite(r[\"name\"])\n",
   "id": "3a5caa37f696a3da",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 6) Load CSVs",
   "id": "a4edd4fdfca4a591"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:58:12.818268Z",
     "start_time": "2025-08-18T01:58:12.800198Z"
    }
   },
   "source": "\ndef load_tables(files: List[Path]):\n    tables = []\n    for p in files:\n        df = pd.read_csv(p)\n        part = col(df, \"metaschema_partition\")  # still captured for debugging/visibility\n        partition = None\n        if part:\n            vals = df[part].dropna().astype(str).unique().tolist()\n            partition = vals[0] if vals else None\n        tables.append({\"path\": p, \"df\": df, \"partition\": partition})\n    return tables\n\ntables = load_tables(CSV_FILES)\nfor t in tables:\n    print(f\"{t['path'].name} :: partition={t['partition']} :: rows={len(t['df'])}\")\n",
   "id": "53244fa1db4bbaaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP Meta-Schema 0.0.3-metaschema-abstract-value-types.csv :: partition=abstract_value_types :: rows=9\n",
      "MAP Meta-Schema 0.0.3-metaschema-concrete-value-types.csv :: partition=concrete_value_types :: rows=27\n",
      "MAP Meta-Schema 0.0.3-metaschema-keyrules-schema.csv :: partition=keyrules :: rows=8\n",
      "MAP Meta-Schema 0.0.3-metaschema-property-types.csv :: partition=property_types :: rows=21\n",
      "MAP Meta-Schema 0.0.3-metaschema-relationship-types.csv :: partition=relationship_types :: rows=38\n",
      "MAP Meta-Schema 0.0.3-metaschema-root.csv :: partition=root :: rows=14\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 7) Build indexes from any suitable tables",
   "id": "d7d00f09892d3c0a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:58:12.842146Z",
     "start_time": "2025-08-18T01:58:12.834076Z"
    }
   },
   "source": "\nkey_rules = KeyRuleEngine()\ninverses = InverseIndex()\n\ndef detect_keyrules(df: pd.DataFrame):\n    c_type = col(df, \"holon_type\", \"type\", \"type_name\")\n    c_fmt = col(df, \"format\")\n    c_props = col(df, \"property_names\")\n    if not (c_type and c_fmt): return\n    for _, row in df.iterrows():\n        ht = str(row.get(c_type, \"\")).strip()\n        fmt = str(row.get(c_fmt, \"\")).strip() or None\n        props = list_norm(row.get(c_props)) if c_props else []\n        if fmt and fmt.lower() in {\"none\", \"none.keyruletype\"}: fmt = None\n        if ht: key_rules.register(ht, fmt, props)\n\ndef detect_inverses(df: pd.DataFrame):\n    c_rel = col(df, \"relationship_name\"); c_inv = col(df, \"inverse_name\", \"inverse_of\")\n    if not (c_rel and c_inv): return\n    for _, row in df.iterrows():\n        declared = str(row.get(c_rel, \"\")).strip(); inv = str(row.get(c_inv, \"\")).strip()\n        if declared and inv: inverses.declare_pair(declared, inv)\n\nfor t in tables:\n    df = t[\"df\"]\n    detect_keyrules(df)\n    detect_inverses(df)\n\nprint(\"KeyRule entries:\", len(key_rules.rules))\nprint(\"Inverse pairs:\", len(inverses.inverse_of))\n",
   "id": "2c66905d577d7062",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyRule entries: 0\n",
      "Inverse pairs: 0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 8) Row mappers (use CSV 'type' to pick JSON type)",
   "id": "7af160b2e7a8cf80"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:58:12.858276Z",
     "start_time": "2025-08-18T01:58:12.853520Z"
    }
   },
   "source": [
    "\n",
    "def relationship_from_column(h: Holon, df_row: pd.Series, csv_col_name: str, rel_name: str):\n",
    "    raw = df_row.get(csv_col_name)\n",
    "    if pd.isna(raw) or raw == \"\": return\n",
    "    targets = [ref(v) for v in list_norm(raw)]\n",
    "    if not targets: return\n",
    "    h.add_relationship(rel_name, targets[0] if len(targets) == 1 else targets)\n",
    "\n",
    "def copy_properties_excluding(h: Holon, df_row: pd.Series, exclude: set):\n",
    "    exclude_lower = {e.lower() for e in exclude}\n",
    "    for c in df_row.index:\n",
    "        if c.lower() in exclude_lower: continue\n",
    "        val = df_row.get(c)\n",
    "        if pd.isna(val): continue\n",
    "        h.add_property(c, val)\n",
    "\n",
    "def build_row(df_row: pd.Series) -> Holon:\n",
    "    # Determine JSON 'type' from CSV 'type' column (row-wise)\n",
    "    tcol = col(df_row.to_frame().T, \"type\")\n",
    "    csv_type_val = (str(df_row.get(tcol, \"\")).strip() if tcol else \"\")\n",
    "    json_type = f\"#{csv_type_val}\"\n",
    "\n",
    "    # Determine key: prefer 'key', else 'type_name', else 'schema_name'\n",
    "    key_col = col(df_row.to_frame().T, \"key\") or col(df_row.to_frame().T, \"type_name\") or col(df_row.to_frame().T, \"schema_name\")\n",
    "    key_val = str(df_row.get(key_col, \"\")).strip() if key_col else \"\"\n",
    "    h = Holon(type_ref=json_type, key=(key_val or None))\n",
    "\n",
    "    # Map relationship columns → relationships\n",
    "    for csv_col, rel_name in REL_COL_TO_NAME.items():\n",
    "        c = col(df_row.to_frame().T, csv_col)\n",
    "        if c: relationship_from_column(h, df_row, c, rel_name)\n",
    "\n",
    "    # Copy other scalars except reserved/relationship columns\n",
    "    copy_properties_excluding(h, df_row, RESERVED_COLS)\n",
    "    return h\n"
   ],
   "id": "ba1def123f1c7273",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 9) Assemble holons and enforce rules",
   "id": "bf1a36126c11bf3d"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:58:12.920737Z",
     "start_time": "2025-08-18T01:58:12.868455Z"
    }
   },
   "source": "\nerrors: List[str] = []\nfile_holons: Dict[str, List[Holon]] = {}\nall_holons: List[Holon] = []\n\ndef enforce_rules(h: Holon):\n    rewrite_relationship_names(h, inverses)\n    errors.extend(key_rules.validate(h))\n    for bad in (\"key\",\"type\"):\n        if bad in h.properties: h.properties.pop(bad, None)\n\nfor t in tables:\n    hs = []\n    for _, row in t[\"df\"].iterrows():\n        h = build_row(row)\n        enforce_rules(h)\n        hs.append(h)\n    file_holons[t[\"path\"].name] = hs\n    all_holons.extend(hs)\n\nprint(f\"Built {len(all_holons)} holon(s) from {len(tables)} CSV file(s).\")\nprint(\"Preview first 3 holons:\")\nfor h in all_holons[:3]:\n    print(json.dumps(h.to_json(), indent=2)[:600], \"...\")\n",
   "id": "928ede6197c4602f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 117 holon(s) from 6 CSV file(s).\n",
      "Preview first 3 holons:\n",
      "{\n",
      "  \"key\": \"MetaValueType\",\n",
      "  \"type\": \"#TypeDescriptor\",\n",
      "  \"properties\": {\n",
      "    \"is_abstract_type\": true,\n",
      "    \"type_name\": \"MetaValueType\",\n",
      "    \"type_name_plural\": \"MetaValueTypes\",\n",
      "    \"display_name\": \"Meta Value Type\",\n",
      "    \"display_name_plural\": \"Meta Value Types\",\n",
      "    \"instance_type_kind\": \"Holon\"\n",
      "  },\n",
      "  \"relationships\": [\n",
      "    {\n",
      "      \"name\": \"ComponentOf\",\n",
      "      \"target\": {\n",
      "        \"$ref\": \"#MAP Metaschema-v0.0.2\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Extends\",\n",
      "      \"target\": {\n",
      "        \"$ref\": \"#MetaHolonType\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"UsesKeyRule\",\n",
      "      \"target\": {\n",
      "        \"$ref\" ...\n",
      "{\n",
      "  \"key\": \"ValueType\",\n",
      "  \"type\": \"#TypeDescriptor\",\n",
      "  \"properties\": {\n",
      "    \"is_abstract_type\": true,\n",
      "    \"type_name\": \"ValueType\",\n",
      "    \"type_name_plural\": \"ValueTypes\",\n",
      "    \"display_name\": \"Value Type\",\n",
      "    \"display_name_plural\": \"Value Types\",\n",
      "    \"instance_type_kind\": \"Holon\"\n",
      "  },\n",
      "  \"relationships\": [\n",
      "    {\n",
      "      \"name\": \"ComponentOf\",\n",
      "      \"target\": {\n",
      "        \"$ref\": \"#MAP Metaschema-v0.0.2\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Extends\",\n",
      "      \"target\": {\n",
      "        \"$ref\": \"#MetaValueType\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"UsesKeyRule\",\n",
      "      \"target\": {\n",
      "        \"$ref\": \"#TypeNameRule.Forma ...\n",
      "{\n",
      "  \"key\": \"StringValueType\",\n",
      "  \"type\": \"#TypeDescriptor\",\n",
      "  \"properties\": {\n",
      "    \"is_abstract_type\": true,\n",
      "    \"type_name\": \"StringValueType\",\n",
      "    \"type_name_plural\": \"StringValueTypes\",\n",
      "    \"display_name\": \"StringValueType\",\n",
      "    \"display_name_plural\": \"StringValueTypes\",\n",
      "    \"description\": \"Abstract type for string-based value types.\"\n",
      "  },\n",
      "  \"relationships\": [\n",
      "    {\n",
      "      \"name\": \"ComponentOf\",\n",
      "      \"target\": {\n",
      "        \"$ref\": \"#MAP Metaschema-v0.0.2\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Extends\",\n",
      "      \"target\": {\n",
      "        \"$ref\": \"#ValueType\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"UsesKeyRule\", ...\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 10) Export JSON",
   "id": "8439ada288a01167"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:58:12.943498Z",
     "start_time": "2025-08-18T01:58:12.931727Z"
    }
   },
   "source": [
    "# --- Export JSON (with cross-file #ref resolution for by-file mode) ---\n",
    "\n",
    "def keys_defined_by(holons: List[Holon]) -> set:\n",
    "    \"\"\"Return the set of keys defined by a holon list.\"\"\"\n",
    "    return {h.key for h in holons if h.key}\n",
    "\n",
    "def refs_from_holons(holons: List[Holon]) -> set:\n",
    "    \"\"\"\n",
    "    Collect ONLY local #... references used by these holons.\n",
    "    We intentionally ignore id:/@/ext: forms here (can't be resolved via local files).\n",
    "    \"\"\"\n",
    "    used = set()\n",
    "    def walk(x):\n",
    "        if isinstance(x, dict):\n",
    "            if \"$ref\" in x and isinstance(x[\"$ref\"], str):\n",
    "                s = x[\"$ref\"]\n",
    "                if s.startswith(\"#\"):\n",
    "                    used.add(s[1:])  # strip leading '#'\n",
    "            for v in x.values():\n",
    "                walk(v)\n",
    "        elif isinstance(x, list):\n",
    "            for v in x:\n",
    "                walk(v)\n",
    "    for h in holons:\n",
    "        walk(h.to_json())\n",
    "    return used\n",
    "\n",
    "def greedy_load_with(unresolved: set, providers: dict) -> (list, set):\n",
    "    \"\"\"\n",
    "    Greedy set cover: pick files that cover the most remaining unresolved keys\n",
    "    until none remain or no progress can be made.\n",
    "      unresolved: set[str] of keys needed\n",
    "      providers:  {filename(str) -> set[str] keys_defined_in_that_file}\n",
    "    Returns (selected_filenames_list, still_unresolved_set).\n",
    "    \"\"\"\n",
    "    remaining = set(unresolved)\n",
    "    selected = []\n",
    "    # Work on a shallow copy so we can pop chosen providers\n",
    "    avail = dict(providers)\n",
    "    while remaining:\n",
    "        best_file, best_cover = None, 0\n",
    "        for fname, kset in avail.items():\n",
    "            cover = len(remaining & kset)\n",
    "            if cover > best_cover:\n",
    "                best_file, best_cover = fname, cover\n",
    "        if best_cover == 0:\n",
    "            break  # no further progress possible\n",
    "        selected.append(best_file)\n",
    "        remaining -= avail[best_file]\n",
    "        avail.pop(best_file, None)\n",
    "    return selected, remaining\n",
    "\n",
    "paths = []\n",
    "errors_found = False  # track whether we printed any unresolved-ref errors\n",
    "\n",
    "if EXPORT_MODE == \"single\":\n",
    "    # All holons in one export: unresolved #refs should be zero if all data is local.\n",
    "    all_defined = keys_defined_by(all_holons)\n",
    "    all_used = refs_from_holons(all_holons)\n",
    "    leftover = sorted(all_used - all_defined)\n",
    "\n",
    "    meta = dict(EXPORT_META)\n",
    "    meta[\"source_files\"] = [t[\"path\"].name for t in tables]\n",
    "    meta[\"load_with\"] = []  # No need to add anything in single-file mode\n",
    "\n",
    "    if leftover:\n",
    "        errors_found = True\n",
    "        print(\"ERROR: unresolved #refs in single export:\", leftover)\n",
    "\n",
    "    export_obj = {\"meta\": meta, \"holons\": [h.to_json() for h in all_holons]}\n",
    "    out = OUTPUT_DIR / \"export.holons.json\"\n",
    "    out.write_text(json.dumps(export_obj, indent=2), encoding=\"utf-8\")\n",
    "    paths.append(out)\n",
    "\n",
    "elif EXPORT_MODE == \"by-file\":\n",
    "    # Precompute providers: which file defines which keys?\n",
    "    providers_by_file = {t[\"path\"].name: keys_defined_by(file_holons.get(t[\"path\"].name, [])) for t in tables}\n",
    "\n",
    "    for t in tables:\n",
    "        fname = t[\"path\"].name\n",
    "        hs = file_holons.get(fname, [])\n",
    "\n",
    "        # Local needs vs. local supply\n",
    "        used = refs_from_holons(hs)\n",
    "        own_keys = providers_by_file.get(fname, set())\n",
    "        unresolved = used - own_keys\n",
    "\n",
    "        # Build provider map excluding self\n",
    "        others = {other_name: kset for other_name, kset in providers_by_file.items() if other_name != fname}\n",
    "\n",
    "        # Greedy cover: pick other files to resolve unresolved refs (only #refs)\n",
    "        selected_files, remaining = greedy_load_with(unresolved, others)\n",
    "\n",
    "        meta = dict(EXPORT_META)\n",
    "        meta[\"source_files\"] = [fname]\n",
    "        # Only include files that actually resolve refs; convert input CSV names -> output JSON names\n",
    "        meta[\"load_with\"] = [Path(sf).stem + \".json\" for sf in selected_files]\n",
    "\n",
    "        if remaining:\n",
    "            errors_found = True\n",
    "            print(f\"ERROR: unresolved #refs in '{fname}': {sorted(remaining)}\")\n",
    "\n",
    "        export_obj = {\"meta\": meta, \"holons\": [h.to_json() for h in hs]}\n",
    "        out = OUTPUT_DIR / (t[\"path\"].stem + \".json\")\n",
    "        out.write_text(json.dumps(export_obj, indent=2), encoding=\"utf-8\")\n",
    "        paths.append(out)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"EXPORT_MODE must be 'single' or 'by-file'\")\n",
    "\n",
    "print(\"Wrote\", len(paths), \"file(s):\")\n",
    "for p in paths:\n",
    "    print(\" -\", p.name)\n",
    "\n",
    "if errors_found:\n",
    "    print(\"\\n✖ One or more exports still contain unresolved local (#...) references. See ERROR lines above.\")\n",
    "else:\n",
    "    print(\"\\n✓ All local (#...) references were resolved within the chosen export mode.\")\n"
   ],
   "id": "8bc911fe6ebf8429",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 6 file(s):\n",
      " - MAP Meta-Schema 0.0.3-metaschema-abstract-value-types.json\n",
      " - MAP Meta-Schema 0.0.3-metaschema-concrete-value-types.json\n",
      " - MAP Meta-Schema 0.0.3-metaschema-keyrules-schema.json\n",
      " - MAP Meta-Schema 0.0.3-metaschema-property-types.json\n",
      " - MAP Meta-Schema 0.0.3-metaschema-relationship-types.json\n",
      " - MAP Meta-Schema 0.0.3-metaschema-root.json\n",
      "\n",
      "✓ All local (#...) references were resolved within the chosen export mode.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 11) Diagnostics",
   "id": "1c58ee74d7439b8f"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:58:12.949918Z",
     "start_time": "2025-08-18T01:58:12.948035Z"
    }
   },
   "source": "\nprint(\"Errors:\", len(errors))\nfor e in errors[:25]: print(\"-\", e)\nif len(errors) > 25: print(\"... (+\", len(errors)-25, \"more)\")\n",
   "id": "cedcef2d2c9910bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 0\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
